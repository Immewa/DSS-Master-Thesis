# -*- coding: utf-8 -*-
"""Thesis_goedepunten_testing_round2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1a57f8A7LhKHxckHvifLfJjlxwXOcbBqu
"""

from google.colab import drive
drive.mount("/content/drive", force_remount=True)

!pip install sentence_transformers

!pip install pandas plotnine

import os
import numpy as np
import pandas as pd

#pre-processing
from nltk import word_tokenize
from nltk.stem.snowball import SnowballStemmer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer

#model
import tensorflow as tf
import transformers
import torch
import sklearn
import plotly as py
import plotly.graph_objs as go
import matplotlib.pyplot as plt

from transformers import BertTokenizer, BertModel, TFAutoModel, AutoTokenizer
from transformers import RobertaTokenizer, RobertaForSequenceClassification
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.neighbors import KNeighborsClassifier
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot
from plotnine import *

#Define stopword filter
def filter_stopwords(sentence, stopwords):
  filter =  [word for word in sentence.split() if word not in stopwords]
  return " ".join(filter)

#Define non-numeric characters deletion
def alpha(sentence):
  sentence = sentence.split()
  filter = [word for word in sentence if word.isalpha() != False]
  return " ".join(filter)

#Define category counter
def count_categories(list):
  counter = 0
  output = list[0]

  for i in list:
    curr_frequency = list.count(i)
    if (curr_frequency > counter):
      counter = curr_frequency
      output = i

  return output

#Import all data
data = pd.read_excel(r"/content/drive/My Drive/Files/NPS_goede_punten.xlsx")
ex_data = pd.read_excel(r"/content/drive/My Drive/Files/goedepunten_voorbeelden_uitdata.xlsx")
ex_art = pd.read_excel(r"/content/drive/My Drive/Files/goedepunten_voorbeelden_artificial.xlsx")
test_set = pd.read_excel(r"/content/drive/My Drive/Files/test_set_final.xlsx", sheet_name = 'good')[['sentence', 'final_v2']]
stopwords = np.loadtxt("/content/drive/My Drive/Files/stopwords.txt", delimiter = ",", dtype = str)
all_sentences = ex_art['all_sentences']
category = ex_art['category_v2']

category

"""**Data cleaning and preprocessing**"""

#Data cleaning and preprocessing: make sure all text is str, make lower case and delete irrelevant punctuation and stopwords
data['Wat vond u goed aan de dienst?'] = data['Wat vond u goed aan de dienst?'].astype(str)
goodpoints = np.asarray(data["Wat vond u goed aan de dienst?"])

for nr in range(len(goodpoints)):
  goodpoints[nr] = alpha(filter_stopwords(goodpoints[nr], stopwords)).lower()

for nr in range(len(all_sentences)):
  all_sentences[nr] = alpha(filter_stopwords(all_sentences[nr], stopwords)).lower()

for nr in range(len(test_set)):
  test_set['sentence'][nr] = alpha(filter_stopwords(test_set['sentence'][nr], stopwords)).lower()

"""**Multilingual model 2**"""

from sentence_transformers import SentenceTransformer, util
import torch

embedder = SentenceTransformer('distiluse-base-multilingual-cased-v2')

#Make example sentence embeddings
multilingual_embeddings = embedder.encode(all_sentences, convert_to_numpy=True)
multilingual_embeddings.shape

#Make test set embeddings
input_sentences = list(test_set['sentence'])
input_embedding = embedder.encode(input_sentences, convert_to_numpy=True)
input_embeddings = np.reshape(input_embedding, (len(test_set['sentence']),512))
input_embeddings.shape

#Calculate similarities
similarities = cosine_similarity(input_embeddings, multilingual_embeddings)

#Print neighbors and output
i = 0
for similarity in similarities[0:50]:
  neighbors = np.argsort(np.reshape(similarity, 90))[-7:][::-1]
  print("Input sentence: ", input_sentences[i], "\n")
  print("Most similar sentences:")

  #get categories
  categories = []
  k = 1
  for j in neighbors:
    if category[j] == 'arts' or category[j] == 'negatief' or category[j] == 'expertise' or category[j] == 'advies' or category[j] == 'gesprek':
      print(k, ".", "Similarity: ", round(similarities[i][j], 3), "\tCategory: ", category[j], "  \t\tSentence: ", all_sentences[j])
    else:
      print(k, ".", "Similarity: ", round(similarities[i][j], 3), "\tCategory: ", category[j], "\tSentence: ", all_sentences[j])
    categories.append(category[j])
    k += 1

  #calculate output
  output = count_categories(categories)
  print("\ny_pred = ", output)
  y = test_set['final_v2'][i]
  print("y_true = ", y, "\n\n")

  i += 1

#Get predictions
predictions = []
for similarity in similarities:
  neighbors = np.argsort(np.reshape(similarity, 90))[-7:][::-1]
  categories = []
  for j in neighbors:
    categories.append(category[j])
  x = count_categories(categories)
  predictions.append(x)

#Get evaluation measures
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

y_true = test_set['final_v2']
y_pred = predictions
labels = ['anders', 'arts', 'gesprek/advies', 'goed concept', 'negatief', 'openingstijden', 'reactietijd']

cm = confusion_matrix(y_true, y_pred, labels = labels)
report = classification_report(y_true, y_pred)
print(report)

#Plot confusion matrix
import seaborn as sns
import matplotlib.pyplot as plt     
from matplotlib.pyplot import figure
ax= plt.subplot()
sns.heatmap(cm, annot=True, ax = ax); #annot=True to annotate cells
sns.set(rc={'figure.figsize':(6,6)})

#Labels, title and ticks
ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels');
ax.set_title('Confusion Matrix'); 
ax.xaxis.set_ticklabels(labels, rotation = 89); ax.yaxis.set_ticklabels(labels, rotation = 0);

"""**Multilingual visualization 2**"""

#Dimensionality reduction
#source https://towardsdatascience.com/elmo-contextual-language-embedding-335de2268604
pca = PCA(n_components=50)
y = pca.fit_transform(multilingual_embeddings)
y = TSNE(n_components=2).fit_transform(y)

plot_data = pd.DataFrame({'x':y[:,0], 'y':y[:,1], 'Category':category})
ggplot(plot_data, aes(x='x', y='y', color='Category')) + geom_point(size = 3)

"""# **All sentences**

"""

len(goodpoints) #there are 11101 sentences to classify

from sentence_transformers import SentenceTransformer, util
import torch

embedder = SentenceTransformer('distiluse-base-multilingual-cased-v2')

#Make example sentence embeddings
multilingual_embeddings = embedder.encode(all_sentences, convert_to_numpy=True)
multilingual_embeddings.shape

#Make all sentence embeddings
input_sentences = goodpoints
input_embedding = embedder.encode(input_sentences, convert_to_numpy=True)
input_embeddings = np.reshape(input_embedding, (len(goodpoints),512))
input_embeddings.shape

#Calculate similarities
similarities = cosine_similarity(input_embeddings, multilingual_embeddings)

#Print neighbors and output
i = 0
outputs = []
for similarity in similarities:
  neighbors = np.argsort(np.reshape(similarity, 90))[-7:][::-1]
  #print("Input sentence: ", input_sentences[i], "\n")
  #print("Most similar sentences:")


  #get categories
  categories = []
  k = 1
  for j in neighbors:
    #if category[j] == 'geen goed concept' or category[j] == 'goed gesprek/advies' or category[j] == 'goed concept':
      #print(k, ".", "Similarity: ", round(similarities[i][j], 3), "\tCategory: ", category[j], "\t\tSentence: ", all_sentences[j])
    #else:
      #print(k, ".", "Similarity: ", round(similarities[i][j], 3), "\tCategory: ", category[j], "\tSentence: ", all_sentences[j])
    categories.append(category[j])
    k += 1

  #calculate output
  output = count_categories(categories)
  outputs.append(output)
  #print("\ny_pred = ", output)
  #y = test_set['final_v2'][i]
  #print("y_true = ", y, "\n\n")
  i += 1

set(outputs)

value_counts

#get value counts
plot_data = pd.DataFrame({'x':outputs})
value_counts = plot_data['x'].value_counts()
df_value_counts = pd.DataFrame({'Category':['gesprek/advies', 'reactietijd', 'negatief', 'goed concept', 'arts', 'openingstijden'], 'Count':[value for value in value_counts], 'Percentage':[round(value/len(outputs),2) for value in value_counts]})

df_value_counts

import seaborn as sns
import matplotlib.pyplot as plt     
from matplotlib.pyplot import figure
#colors = ["#FF0B04", "#4374B3"]
#customPalette = sns.set_palette(sns.color_palette(colors))
#sns.set_palette(customPalette)
sns.set_style("ticks")
sns.set_palette("Spectral")
sns.barplot(x='Category', y = 'Count', data = df_value_counts)
sns.set(rc={'figure.figsize':(10,10)})
sns.despine()
plt.xticks(rotation=90)