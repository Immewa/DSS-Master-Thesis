# -*- coding: utf-8 -*-
"""Thesis_sentiment_analysis BERTje

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ejkSSbXETXUdeod75fo6CG_RdV5tWYGG
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install transformers
!pip install plotnine

import numpy as np
import pandas as pd
import tensorflow as tf

import transformers
import sklearn

from transformers import BertTokenizer, BertModel, TFAutoModel, AutoTokenizer
from transformers import RobertaTokenizer, RobertaForSequenceClassification
from sklearn.metrics.pairwise import cosine_similarity
from plotnine import * 
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE

#Define stopword filter
def filter_stopwords(sentence, stopwords):
  filter =  [word for word in sentence.split() if word not in stopwords]
  return " ".join(filter)

#Define non-numeric characters deletion
def alpha(sentence):
  sentence = sentence.split()
  filter = [word for word in sentence if word.isalpha() != False]
  return " ".join(filter)

#Define category counter
def count_categories(list):
  counter = 0
  output = list[0]

  for i in list:
    curr_frequency = list.count(i)
    if (curr_frequency > counter):
      counter = curr_frequency
      output = i

  return output

#Load data
sentences = pd.read_excel(r"/content/drive/My Drive/Files/sentiment_forsentenceembeddings.xlsx")
x_train = sentences['sentence'].astype(str)
y_train = sentences['sentiment']
test = pd.read_csv(r"/content/drive/MyDrive/Files/test.tsv", sep = ";")
x_test = test['text'].astype(str)
y_test = test['sentiment']
stopwords = np.loadtxt("/content/drive/My Drive/Files/stopwords.txt", delimiter = ",", dtype = str)

#Preprocess data
for nr in range(len(x_train)):
  x_train[nr] = alpha(filter_stopwords(x_train[nr], stopwords)).lower()

for nr in range(len(x_test)):
  x_test[nr] = alpha(filter_stopwords(x_test[nr], stopwords)).lower()

#BERTje
tokenizer_bertje = AutoTokenizer.from_pretrained("wietsedv/bert-base-dutch-cased")
model_bertje = TFAutoModel.from_pretrained("wietsedv/bert-base-dutch-cased")

all_tokenized = tokenizer_bertje(list(x_train), return_tensors = "tf", padding = True)
all_embeddings = model_bertje(all_tokenized)[0][:,0,:].numpy()

all_embeddings.shape

input_sentences = list(x_test)
input_tokenized = tokenizer_bertje(input_sentences, return_tensors = "tf", padding = True)
input_embedding = model_bertje(input_tokenized)[0][:,0,:].numpy()

similarities = cosine_similarity(input_embedding, all_embeddings)

#Print neighbors and output
i = 0
outputs = []
for similarity in similarities:
  neighbors = np.argsort(np.reshape(similarity, 300))[-3:][::-1]
  #print("Input sentence: ", input_sentences[i], "\n")
  #print("Most similar sentences:")


  #get categories
  categories = []
  k = 1
  for j in neighbors:
    #if category[j] == 'geen goed concept' or category[j] == 'goed gesprek/advies' or category[j] == 'goed concept':
      #print(k, ".", "Similarity: ", round(similarities[i][j], 3), "\tCategory: ", category[j], "\t\tSentence: ", all_sentences[j])
    #else:
      #print(k, ".", "Similarity: ", round(similarities[i][j], 3), "\tCategory: ", category[j], "\tSentence: ", all_sentences[j])
    categories.append(y_train[j])
    k += 1

  #calculate output
  output = count_categories(categories)
  outputs.append(output)
  #print("\ny_pred = ", output)
  #y = test_set['final_v2'][i]
  #print("y_true = ", y, "\n\n")
  i += 1

set(outputs)

#get value counts
plot_data = pd.DataFrame({'x':outputs})
value_counts = plot_data['x'].value_counts()
df_value_counts = pd.DataFrame({'Category':['positief','negatief','neutraal'], 'Count':[value for value in value_counts], 'Percentage':[round(value/len(outputs),2) for value in value_counts]})

value_counts

#get predictions

#Get predictions
predictions = []
for similarity in similarities:
  neighbors = np.argsort(np.reshape(similarity, 300))[-3:][::-1]
  categories = []
  for j in neighbors:
    categories.append(y_train[j])
  x = count_categories(categories)
  predictions.append(x)

#Get evaluation measures
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

y_true = y_test
y_pred = predictions
labels = ['positief', 'neutraal', 'negatief']

cm = confusion_matrix(y_true, y_pred, labels = labels)
report = classification_report(y_true, y_pred)
print(report)



#Plot confusion matrix
import seaborn as sns
import matplotlib.pyplot as plt     
from matplotlib.pyplot import figure
ax= plt.subplot()
sns.heatmap(cm, annot=True, ax = ax, fmt = 'g'); #annot=True to annotate cells
sns.set(rc={'figure.figsize':(6,6)})

#Labels, title and ticks
ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels');
ax.set_title('Confusion Matrix'); 
ax.xaxis.set_ticklabels(labels, rotation = 89); ax.yaxis.set_ticklabels(labels, rotation = 0);

#Dimensionality reduction
#source https://towardsdatascience.com/elmo-contextual-language-embedding-335de2268604
pca = PCA(n_components=50)
y = pca.fit_transform(all_embeddings)
y = TSNE(n_components=2).fit_transform(y)

plot_data = pd.DataFrame({'x':y[:,0], 'y':y[:,1], 'Category':y_train})

ggplot(plot_data, aes(x='x', y='y', color='Category')) + geom_point(size = 3)