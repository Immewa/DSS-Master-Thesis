# -*- coding: utf-8 -*-
"""Thesis_toelichting_mutliclass_classifier

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BDoKwkYW8iWfGbqQ2G4R7vud8Ao_VD7J
"""

from google.colab import drive
drive.mount("/content/drive", force_remount=True)

!pip install sentence_transformers
!pip install pandas plotnine

import os
import numpy as np
import pandas as pd

#pre-processing
from nltk import word_tokenize
from nltk.stem.snowball import SnowballStemmer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer

#model
import tensorflow as tf
import torch
import sklearn
import plotly as py
import plotly.graph_objs as go
import matplotlib.pyplot as plt

from transformers import BertTokenizer, BertModel, TFAutoModel, AutoTokenizer
from transformers import RobertaTokenizer, RobertaForSequenceClassification
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.neighbors import KNeighborsClassifier
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot
from plotnine import *

#Define stopword filter
def filter_stopwords(sentence, stopwords):
  filter =  [word for word in sentence.split() if word not in stopwords]
  return " ".join(filter)

#Define non-numeric characters deletion
def alpha(sentence):
  sentence = sentence.split()
  filter = [word for word in sentence if word.isalpha() != False]
  return " ".join(filter)

#Define category counter
def count_categories(list):
  counter = 0
  output = list[0]

  for i in list:
    curr_frequency = list.count(i)
    if (curr_frequency > counter):
      counter = curr_frequency
      output = i

  return output

#Define multiclass category counter
def count_categories_multiclass(neighbors, similarity, categories):
  output = []
  i = 0
  for neighbor in neighbors:
      if similarity[neighbor] >= .7:
        output.append(categories[i])
        i += 1

  if len(output) == 0:
    output.append('other')
    
  return np.array(set(output))

#Import all data
data = pd.read_excel(r"/content/drive/My Drive/Files/NPS_toelichting.xlsx")

ex_art = pd.read_excel(r"/content/drive/My Drive/Files/toelichting_voorbeelden_artificial.xlsx")

test_set = pd.read_excel(r"/content/drive/My Drive/Files/test_set_final.xlsx", sheet_name = 'explain')[['sentence', 'final_v2']] #combined categories

all_sentences = ex_art['exp_allsentences'] 
category = ex_art['exp_categorie_v2'] #combined categories

stopwords = np.loadtxt("/content/drive/My Drive/Files/stopwords.txt", delimiter = ",", dtype = str)

#Data cleaning and preprocessing: make sure all text is str, make lower case and delete irrelevant punctuation and stopwords
data['Zou u uw antwoord kunnen toelichten?'] = data['Zou u uw antwoord kunnen toelichten?'].astype(str)
explanations = np.asarray(data["Zou u uw antwoord kunnen toelichten?"])

for nr in range(len(explanations)):
  explanations[nr] = alpha(filter_stopwords(explanations[nr], stopwords)).lower()

for nr in range(len(all_sentences)):
  all_sentences[nr] = alpha(filter_stopwords(all_sentences[nr], stopwords)).lower()

for nr in range(len(test_set)):
  test_set['sentence'][nr] = alpha(filter_stopwords(test_set['sentence'][nr], stopwords)).lower()

"""**Multilingual model 3**"""

from sentence_transformers import SentenceTransformer, util
import torch

embedder = SentenceTransformer('xlm-r-distilroberta-base-paraphrase-v1')

#Make example sentence embeddings
multilingual_embeddings = embedder.encode(all_sentences, convert_to_numpy=True)
multilingual_embeddings.shape

#Make test set embeddings
input_sentences = list(test_set['sentence'])
input_embedding = embedder.encode(input_sentences, convert_to_numpy=True)
input_embeddings = np.reshape(input_embedding, (len(test_set['sentence']),768))
input_embeddings.shape

#Calculate similarities
similarities = cosine_similarity(input_embeddings, multilingual_embeddings)

neighbors

similarities[0][98]

#Print neighbors and output
i = 0
for similarity in similarities[:1]:
  neighbors = np.argsort(np.reshape(similarity, 100))[-5:][::-1]
  print("Input sentence: ", input_sentences[i], "\n")
  print("Most similar sentences:")


  #get categories
  categories = []
  k = 1
  for j in neighbors:
    if category[j] == 'geen goed concept' or category[j] == 'goed gesprek/advies' or category[j] == 'goed concept':
      print(k, ".", "Similarity: ", round(similarities[i][j], 3), "\tCategory: ", category[j], "\t\tSentence: ", all_sentences[j])
    else:
      print(k, ".", "Similarity: ", round(similarities[i][j], 3), "\tCategory: ", category[j], "\tSentence: ", all_sentences[j])
    categories.append(category[j])
    k += 1

  #calculate output
  output = count_categories(categories)
  print("\ny_pred = ", output)
  y = test_set['final_v2'][i]
  print("y_true = ", y, "\n\n")

  i += 1

#Print neighbors and output multiclass
i = 0
for similarity in similarities[:50]:
  neighbors = np.argsort(np.reshape(similarity, 100))[-5:][::-1]
  print("Input sentence: ", input_sentences[i], "\n")
  print("Most similar sentences:")


  #get categories
  categories = []
  k = 1
  for j in neighbors:
    if category[j] == 'geen goed concept' or category[j] == 'goed gesprek/advies' or category[j] == 'goed concept':
      print(k, ".", "Similarity: ", round(similarities[i][j], 3), "\tCategory: ", category[j], "\t\tSentence: ", all_sentences[j])
    else:
      print(k, ".", "Similarity: ", round(similarities[i][j], 3), "\tCategory: ", category[j], "\tSentence: ", all_sentences[j])
    categories.append(category[j])
    k += 1

  #calculate output
  output = count_categories_multiclass(neighbors, similarity, categories)
  print("\ny_pred = ", output)
  y = test_set['final_v2'][i]
  print("y_true = ", y, "\n\n")

  i += 1

#Get predictions
predictions = []
for similarity in similarities:
  neighbors = np.argsort(np.reshape(similarity, 100))[-3:][::-1]
  categories = []
  for j in neighbors:
    categories.append(category[j])
  x = count_categories(categories)
  predictions.append(x)

#Get evaluation measures
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

y_true = test_set['final_v2']
y_pred = predictions
labels = ['geen goed concept',
 'goed concept',
 'goed gesprek/advies',
 'langzame reactie',
 'slecht gesprek/advies',
 'snelle reactie']

cm = confusion_matrix(y_true, y_pred, labels = labels)
report = classification_report(y_true, y_pred)
print(report)

#Plot confusion matrix
import seaborn as sns
import matplotlib.pyplot as plt     
from matplotlib.pyplot import figure
ax= plt.subplot()
sns.heatmap(cm, annot=True, ax = ax); #annot=True to annotate cells
sns.set(rc={'figure.figsize':(6,6)})

#Labels, title and ticks
ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels');
ax.set_title('Confusion Matrix'); 
ax.xaxis.set_ticklabels(labels, rotation = 89); ax.yaxis.set_ticklabels(labels, rotation = 0);

"""**Multilingual visualization 3**"""

#Dimensionality reduction
#source https://towardsdatascience.com/elmo-contextual-language-embedding-335de2268604
pca = PCA(n_components=50)
y = pca.fit_transform(multilingual_embeddings)
y = TSNE(n_components=2).fit_transform(y)

plot_data = pd.DataFrame({'x':y[:,0], 'y':y[:,1], 'Category':category})

ggplot(plot_data, aes(x='x', y='y', color='Category')) + geom_point(size = 3)