# -*- coding: utf-8 -*-
"""Thesis_suggesties_art

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yvoEkR3JmQGwuTRZa9zVOMtIlhK3oPxI
"""

from google.colab import drive
drive.mount("/content/drive", force_remount=True)

!pip install sentence_transformers

!pip install pandas plotnine

!pip install transformers

import os
import numpy as np
import pandas as pd

#pre-processing
from nltk import word_tokenize
from nltk.stem.snowball import SnowballStemmer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer

#model
import tensorflow as tf
import transformers
import torch
import sklearn
import plotly as py
import plotly.graph_objs as go
import matplotlib.pyplot as plt

from transformers import BertTokenizer, BertModel, TFAutoModel, AutoTokenizer
from transformers import RobertaTokenizer, RobertaForSequenceClassification
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.neighbors import KNeighborsClassifier
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot
from plotnine import *

#Define stopword filter
def filter_stopwords(sentence, stopwords):
  filter =  [word for word in sentence.split() if word not in stopwords]
  return " ".join(filter)

#Define non-numeric characters deletion
def alpha(sentence):
  sentence = sentence.split()
  filter = [word for word in sentence if word.isalpha() != False]
  return " ".join(filter)

#Define category counter
def count_categories(list):
  counter = 0
  output = list[0]

  for i in list:
    curr_frequency = list.count(i)
    if (curr_frequency > counter):
      counter = curr_frequency
      output = i

  return output

#Import all data
data = pd.read_excel(r"/content/drive/My Drive/Files/NPS_verbeterpunten.xlsx")
ex_art = pd.read_excel(r"/content/drive/My Drive/Files/suggesties_voorbeelden_artificial.xlsx")
test_set = pd.read_excel(r"/content/drive/My Drive/Files/test_set_final.xlsx", sheet_name = 'suggest')[['sentence', 'final']]
stopwords = np.loadtxt("/content/drive/My Drive/Files/stopwords.txt", delimiter = ",", dtype = str)
all_sentences = ex_art['all_sentences']
category = ex_art['category']
labels = 'gesprek', 'geen', 'gebruikerservaring', 'advies', 'bugs', 'reactietijd'

"""**Data exploration**"""

#Exploring raw data
print("Suggestions: ", len(data), " responses")
print("")
print("Minimum date: ", min(data["Datum"]))
print("Maximum date: ", max(data["Datum"]))
print("Duration: ", max(data["Datum"])-min(data["Datum"]))
print("")
print("Columns:")
print(data.columns.tolist())

#Exploring example sentences
print("Number of categories for explanation question:", len(set(category)))
print("Total number of example sentences:", len(all_sentences)) #check that this is number of categories * 10
print("Categories:", set(category))
print()


#Some example sentences:
print("Some examples sentences:")
print(all_sentences[:5])

#Exploring test set
print(test_set.columns)
print(len(test_set))

"""**Data cleaning and preprocessing**"""

#Data cleaning and preprocessing: make sure all text is str, make lower case and delete irrelevant punctuation and stopwords
data['Heeft u nog verbeterpunten/suggesties/tips voor de dienst?'] = data['Heeft u nog verbeterpunten/suggesties/tips voor de dienst?'].astype(str)
suggestions = np.asarray(data["Heeft u nog verbeterpunten/suggesties/tips voor de dienst?"])

for nr in range(len(suggestions)):
  suggestions[nr] = alpha(filter_stopwords(suggestions[nr], stopwords)).lower()

for nr in range(len(all_sentences)):
  all_sentences[nr] = alpha(filter_stopwords(all_sentences[nr], stopwords)).lower()

for nr in range(len(test_set)):
  test_set['sentence'][nr] = alpha(filter_stopwords(test_set['sentence'][nr], stopwords)).lower()

"""**Extract BERTje embeddings**"""

#BERTje
tokenizer_bertje = AutoTokenizer.from_pretrained("wietsedv/bert-base-dutch-cased")
model_bertje = TFAutoModel.from_pretrained("wietsedv/bert-base-dutch-cased")

#tokenize the values using the tokenizer that was developed for the model
#padding = True makes sure all arrays have the same length
#attention_mask will ignore the padded parts of the sentence

#get the hidden states from the model, i.e. the embeddings

#embeddings should be of shape (number of sentences, max sentence length, number of hidden layers in the model)

#Let's slice only the part of the output that we need. That is the output corresponding the first token of each sentence. 
#The way BERT does sentence classification, is that it adds a token called [CLS] (for classification) at the beginning of every sentence. 
#The output corresponding to that token can be thought of as an embedding for the entire sentence.
#[:,0,:] : = all sentences, 0 = only the first position which is CLS, : = all hidden unit outputs
#shape should now be (number of sentences, number of hidden layers)

all_tokenized = tokenizer_bertje(list(all_sentences), return_tensors = "tf", padding = True)
all_embeddings = model_bertje(all_tokenized)[0][:,0,:].numpy()

all_embeddings.shape

"""**KNN BERTje embeddings**"""

#plot word embeddings

input_sentences = list(test_set['sentence'])
input_tokenized = tokenizer_bertje(input_sentences, return_tensors = "tf", padding = True)
input_embedding = model_bertje(input_tokenized)[0][:,0,:].numpy()

similarities = cosine_similarity(input_embedding, all_embeddings)

similarities.shape

#Print neighbors and output
i = 0
for similarity in similarities[0:3]:
  neighbors = np.argsort(np.reshape(similarity, 60))[-5:][::-1]
  print("Input sentence: ", input_sentences[i], "\n")
  print("Most similar sentences:")

  #get categories
  categories = []
  k = 1
  for j in neighbors:
    if category[j] == 'geen' or category[j] == 'bugs' or category[j] == 'gesprek' or category[j] == 'advies' or category[j] == 'reactietijd':
      print(k, ".", "Similarity: ", round(similarities[i][j], 3), "\tCategory: ", category[j], "\t\tSentence: ", all_sentences[j])
    else:
      print(k, ".", "Similarity: ", round(similarities[i][j], 3), "\tCategory: ", category[j], "\tSentence: ", all_sentences[j])
    categories.append(category[j])
    k += 1

  #calculate output
  output = count_categories(categories)
  print("\ny_pred = ", output)
  y = test_set['final'][i]
  print("y_true = ", y, "\n\n")

  i += 1

#Get predictions
predictions = []
for similarity in similarities:
  neighbors = np.argsort(np.reshape(similarity, 60))[-5:][::-1]
  categories = []
  for j in neighbors:
    categories.append(category[j])
  x = count_categories(categories)
  predictions.append(x)

#Get evaluation measures
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

y_true = test_set['final']
y_pred = predictions
labels = ['gesprek', 'geen', 'gebruikerservaring', 'advies', 'bugs', 'reactietijd']

cm = confusion_matrix(y_true, y_pred, labels = labels)
report = classification_report(y_true, y_pred)
print(report)

#Plot confusion matrix
import seaborn as sns
import matplotlib.pyplot as plt     
from matplotlib.pyplot import figure
ax= plt.subplot()
sns.heatmap(cm, annot=True, ax = ax); #annot=True to annotate cells
sns.set(rc={'figure.figsize':(6,6)})

#Labels, title and ticks
ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels');
ax.set_title('Confusion Matrix'); 
ax.xaxis.set_ticklabels(labels, rotation = 89); ax.yaxis.set_ticklabels(labels, rotation = 0);

"""**PCA & Visualization**

"""

#Dimensionality reduction
#source https://towardsdatascience.com/elmo-contextual-language-embedding-335de2268604
pca = PCA(n_components=50)
y = pca.fit_transform(all_embeddings)
y = TSNE(n_components=2).fit_transform(y)

#static visualization
plot_data = pd.DataFrame({'x':y[:,0], 'y':y[:,1], 'Category':category})
ggplot(plot_data, aes(x='x', y='y', color='Category')) + geom_point(size = 3)

"""**Extract RobBERT embeddings**"""

#RobBERT model
tokenizer_robbert = RobertaTokenizer.from_pretrained("pdelobelle/robbert-v2-dutch-base")
model_robbert = RobertaForSequenceClassification.from_pretrained("pdelobelle/robbert-v2-dutch-base")

inputs_robbert = tokenizer_robbert(list(all_sentences), return_tensors = "pt", padding = True)
print(inputs_robbert['input_ids'].shape)
print(inputs_robbert['attention_mask'].shape)

#get the hidden states from the model, i.e. the embeddings
all_tokenized_robbert = tokenizer_robbert(list(all_sentences), return_tensors = "pt", padding = True)
all_embedding_robbert = model_robbert(**all_tokenized_robbert, output_hidden_states = True)[1][0][:][:]
all_embedding_robbert = all_embedding_robbert.detach().numpy()
all_embedding_robbert = np.mean(all_embedding_robbert, axis = 1)

all_embedding_robbert.shape

"""**KNN RobBERT embeddings**

"""

#Sentence embeddings for test set
input_sentences = list(test_set['sentence'])
input_tokenized_robbert = tokenizer_robbert(input_sentences, return_tensors = "pt", padding = True)
input_embedding_robbert = model_robbert(**input_tokenized_robbert, output_hidden_states = True)[1][0][:][:]
input_embedding_robbert = input_embedding_robbert.detach().numpy()
input_embedding_robbert = np.mean(input_embedding_robbert, axis = 1)

similarities = cosine_similarity(input_embedding_robbert, all_embedding_robbert)

#Print neighbors and output
i = 0
for similarity in similarities[0:3]:
  neighbors = np.argsort(np.reshape(similarity, 60))[-5:][::-1]
  print("Input sentence: ", input_sentences[i], "\n")
  print("Most similar sentences:")

  #get categories
  categories = []
  k = 1
  for j in neighbors:
    if category[j] == 'geen' or category[j] == 'bugs' or category[j] == 'gesprek' or category[j] == 'advies' or category[j] == 'reactietijd':
      print(k, ".", "Similarity: ", round(similarities[i][j], 3), "\tCategory: ", category[j], "\t\tSentence: ", all_sentences[j])
    else:
      print(k, ".", "Similarity: ", round(similarities[i][j], 3), "\tCategory: ", category[j], "\tSentence: ", all_sentences[j])
    categories.append(category[j])
    k += 1

  #calculate output
  output = count_categories(categories)
  print("\ny_pred = ", output)
  y = test_set['final'][i]
  print("y_true = ", y, "\n\n")

  i += 1

#Get predictions
predictions = []
for similarity in similarities:
  neighbors = np.argsort(np.reshape(similarity, 60))[-5:][::-1]
  categories = []
  for j in neighbors:
    categories.append(category[j])
  x = count_categories(categories)
  predictions.append(x)

#Get evaluation measures
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

y_true = test_set['final']
y_pred = predictions
labels = ['gesprek', 'geen', 'gebruikerservaring', 'advies', 'bugs', 'reactietijd']

cm = confusion_matrix(y_true, y_pred, labels = labels)
report = classification_report(y_true, y_pred)
print(report)

#Plot confusion matrix
import seaborn as sns
import matplotlib.pyplot as plt     
from matplotlib.pyplot import figure
ax= plt.subplot()
sns.heatmap(cm, annot=True, ax = ax); #annot=True to annotate cells
sns.set(rc={'figure.figsize':(6,6)})

#Labels, title and ticks
ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels');
ax.set_title('Confusion Matrix'); 
ax.xaxis.set_ticklabels(labels, rotation = 89); ax.yaxis.set_ticklabels(labels, rotation = 0);

"""**Visualization RobBERT** """

#Dimensionality reduction
#source https://towardsdatascience.com/elmo-contextual-language-embedding-335de2268604
pca = PCA(n_components=50)
y = pca.fit_transform(all_embedding_robbert)
y = TSNE(n_components=2).fit_transform(y)

plot_data = pd.DataFrame({'x':y[:,0], 'y':y[:,1], 'Category':category})
ggplot(plot_data, aes(x='x', y='y', color='Category')) + geom_point(size = 3)

"""**Multilingual model 1**"""

from sentence_transformers import SentenceTransformer, util
import torch

embedder = SentenceTransformer('xlm-r-bert-base-nli-stsb-mean-tokens')

#Make example sentence embeddings
multilingual_embeddings = embedder.encode(all_sentences, convert_to_numpy=True)
multilingual_embeddings.shape

#Make test set embeddings
input_sentences = list(test_set['sentence'])
input_embedding = embedder.encode(input_sentences, convert_to_numpy=True)
input_embeddings = np.reshape(input_embedding, (len(test_set['sentence']),768))
input_embeddings.shape

similarities = cosine_similarity(input_embeddings, multilingual_embeddings)

#Print neighbors and output
i = 0
for similarity in similarities[0:30]:
  neighbors = np.argsort(np.reshape(similarity, 60))[-5:][::-1]
  print("Input sentence: ", input_sentences[i], "\n")
  print("Most similar sentences:")

  #get categories
  categories = []
  k = 1
  for j in neighbors:
    if category[j] == 'geen' or category[j] == 'bugs' or category[j] == 'gesprek' or category[j] == 'advies' or category[j] == 'reactietijd':
      print(k, ".", "Similarity: ", round(similarities[i][j], 3), "\tCategory: ", category[j], "  \t\tSentence: ", all_sentences[j])
    else:
      print(k, ".", "Similarity: ", round(similarities[i][j], 3), "\tCategory: ", category[j], "\tSentence: ", all_sentences[j])
    categories.append(category[j])
    k += 1

  #calculate output
  output = count_categories(categories)
  print("\ny_pred = ", output)
  y = test_set['final'][i]
  print("y_true = ", y, "\n\n")

  i += 1

#Get predictions
predictions = []
for similarity in similarities:
  neighbors = np.argsort(np.reshape(similarity, 60))[-5:][::-1]
  categories = []
  for j in neighbors:
    categories.append(category[j])
  x = count_categories(categories)
  predictions.append(x)

#Get evaluation measures
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

y_true = test_set['final']
y_pred = predictions
labels = labels

cm = confusion_matrix(y_true, y_pred, labels = labels)
report = classification_report(y_true, y_pred)
print(report)

#Plot confusion matrix
import seaborn as sns
import matplotlib.pyplot as plt     
from matplotlib.pyplot import figure
ax= plt.subplot()
sns.heatmap(cm, annot=True, ax = ax); #annot=True to annotate cells
sns.set(rc={'figure.figsize':(6,6)})

#Labels, title and ticks
ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels');
ax.set_title('Confusion Matrix'); 
ax.xaxis.set_ticklabels(labels, rotation = 89); ax.yaxis.set_ticklabels(labels, rotation = 0);

"""**Multilingual visualization 1**"""

#Dimensionality reduction
#source https://towardsdatascience.com/elmo-contextual-language-embedding-335de2268604
pca = PCA(n_components=50)
y = pca.fit_transform(multilingual_embeddings)
y = TSNE(n_components=2).fit_transform(y)

plot_data = pd.DataFrame({'x':y[:,0], 'y':y[:,1], 'Category':category})
ggplot(plot_data, aes(x='x', y='y', color='Category')) + geom_point(size = 3)

"""**Multilingual model 2**"""

from sentence_transformers import SentenceTransformer, util
import torch

embedder = SentenceTransformer('distiluse-base-multilingual-cased-v2')

#Make example sentence embeddings
multilingual_embeddings = embedder.encode(all_sentences, convert_to_numpy=True)
multilingual_embeddings.shape

#Make test set embeddings
input_sentences = list(test_set['sentence'])
input_embedding = embedder.encode(input_sentences, convert_to_numpy=True)
input_embeddings = np.reshape(input_embedding, (len(test_set['sentence']),512))
input_embeddings.shape

#Calculate similarities
similarities = cosine_similarity(input_embeddings, multilingual_embeddings)

#Print neighbors and output
i = 0
for similarity in similarities[0:3]:
  neighbors = np.argsort(np.reshape(similarity, 60))[-5:][::-1]
  print("Input sentence: ", input_sentences[i], "\n")
  print("Most similar sentences:")

  #get categories
  categories = []
  k = 1
  for j in neighbors:
    if category[j] == 'geen' or category[j] == 'bugs' or category[j] == 'gesprek' or category[j] == 'advies' or category[j] == 'reactietijd':
      print(k, ".", "Similarity: ", round(similarities[i][j], 3), "\tCategory: ", category[j], "  \t\tSentence: ", all_sentences[j])
    else:
      print(k, ".", "Similarity: ", round(similarities[i][j], 3), "\tCategory: ", category[j], "\tSentence: ", all_sentences[j])
    categories.append(category[j])
    k += 1

  #calculate output
  output = count_categories(categories)
  print("\ny_pred = ", output)
  y = test_set['final'][i]
  print("y_true = ", y, "\n\n")

  i += 1

#Get predictions
predictions = []
for similarity in similarities:
  neighbors = np.argsort(np.reshape(similarity, 60))[-5:][::-1]
  categories = []
  for j in neighbors:
    categories.append(category[j])
  x = count_categories(categories)
  predictions.append(x)

#Get evaluation measures
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

y_true = test_set['final']
y_pred = predictions
labels = labels

cm = confusion_matrix(y_true, y_pred, labels = labels)
report = classification_report(y_true, y_pred)
print(report)

#Plot confusion matrix
import seaborn as sns
import matplotlib.pyplot as plt     
from matplotlib.pyplot import figure
ax= plt.subplot()
sns.heatmap(cm, annot=True, ax = ax); #annot=True to annotate cells
sns.set(rc={'figure.figsize':(6,6)})

#Labels, title and ticks
ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels');
ax.set_title('Confusion Matrix'); 
ax.xaxis.set_ticklabels(labels, rotation = 89); ax.yaxis.set_ticklabels(labels, rotation = 0);

"""**Multilingual visualization 2**"""

#Dimensionality reduction
#source https://towardsdatascience.com/elmo-contextual-language-embedding-335de2268604
pca = PCA(n_components=50)
y = pca.fit_transform(multilingual_embeddings)
y = TSNE(n_components=2).fit_transform(y)

plot_data = pd.DataFrame({'x':y[:,0], 'y':y[:,1], 'Category':category})
ggplot(plot_data, aes(x='x', y='y', color='Category')) + geom_point(size = 3)

"""**Multilingual model 3**"""

from sentence_transformers import SentenceTransformer, util
import torch

embedder = SentenceTransformer('xlm-r-distilroberta-base-paraphrase-v1')

#Make example sentence embeddings
multilingual_embeddings = embedder.encode(all_sentences, convert_to_numpy=True)
multilingual_embeddings.shape

#Make test set embeddings
input_sentences = list(test_set['sentence'])
input_embedding = embedder.encode(input_sentences, convert_to_numpy=True)
input_embeddings = np.reshape(input_embedding, (len(test_set['sentence']),768))
input_embeddings.shape

#Calculate similarities
similarities = cosine_similarity(input_embeddings, multilingual_embeddings)

#Print neighbors and output
i = 0
for similarity in similarities[0:3]:
  neighbors = np.argsort(np.reshape(similarity, 60))[-5:][::-1]
  print("Input sentence: ", input_sentences[i], "\n")
  print("Most similar sentences:")

  #get categories
  categories = []
  k = 1
  for j in neighbors:
    if category[j] == 'geen' or category[j] == 'bugs' or category[j] == 'gesprek' or category[j] == 'advies' or category[j] == 'reactietijd':
      print(k, ".", "Similarity: ", round(similarities[i][j], 3), "\tCategory: ", category[j], "  \t\tSentence: ", all_sentences[j])
    else:
      print(k, ".", "Similarity: ", round(similarities[i][j], 3), "\tCategory: ", category[j], "\tSentence: ", all_sentences[j])
    categories.append(category[j])
    k += 1

  #calculate output
  output = count_categories(categories)
  print("\ny_pred = ", output)
  y = test_set['final'][i]
  print("y_true = ", y, "\n\n")

  i += 1

#Get predictions
predictions = []
for similarity in similarities:
  neighbors = np.argsort(np.reshape(similarity, 60))[-5:][::-1]
  categories = []
  for j in neighbors:
    categories.append(category[j])
  x = count_categories(categories)
  predictions.append(x)

#Get evaluation measures
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

y_true = test_set['final']
y_pred = predictions
labels = labels

cm = confusion_matrix(y_true, y_pred, labels = labels)
report = classification_report(y_true, y_pred)
print(report)

#Plot confusion matrix
import seaborn as sns
import matplotlib.pyplot as plt     
from matplotlib.pyplot import figure
ax= plt.subplot()
sns.heatmap(cm, annot=True, ax = ax); #annot=True to annotate cells
sns.set(rc={'figure.figsize':(6,6)})

#Labels, title and ticks
ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels');
ax.set_title('Confusion Matrix'); 
ax.xaxis.set_ticklabels(labels, rotation = 89); ax.yaxis.set_ticklabels(labels, rotation = 0);

"""**Multilingual visualization 3**"""

#Dimensionality reduction
#source https://towardsdatascience.com/elmo-contextual-language-embedding-335de2268604
pca = PCA(n_components=50)
y = pca.fit_transform(multilingual_embeddings)
y = TSNE(n_components=2).fit_transform(y)

plot_data = pd.DataFrame({'x':y[:,0], 'y':y[:,1], 'Category':category})
ggplot(plot_data, aes(x='x', y='y', color='Category')) + geom_point(size = 3)

"""**Decoding input id's**"""

#How to decode the input id's 
encoded = tokenizer("ik vind het een goede service")["input_ids"]
print(encoded)
decoded = tokenizer.decode(encoded)
print(decoded)

"""# **Improving model**"""



#Import all data
data = pd.read_excel(r"/content/drive/My Drive/Files/NPS_verbeterpunten.xlsx")
ex_art = pd.read_excel(r"/content/drive/My Drive/Files/suggesties_voorbeelden_artificial.xlsx")
test_set = pd.read_excel(r"/content/drive/My Drive/Files/test_set_final.xlsx", sheet_name = 'suggest')[['sentence', 'final_v2']]
stopwords = np.loadtxt("/content/drive/My Drive/Files/stopwords.txt", delimiter = ",", dtype = str)
all_sentences = ex_art['all_sentences']
category = ex_art['category_v2']
labels = ['anders', 'bugs', 'geen', 'reactietijd', 'tips']

#Data cleaning and preprocessing: make sure all text is str, make lower case and delete irrelevant punctuation and stopwords
data['Heeft u nog verbeterpunten/suggesties/tips voor de dienst?'] = data['Heeft u nog verbeterpunten/suggesties/tips voor de dienst?'].astype(str)
suggestions = np.asarray(data["Heeft u nog verbeterpunten/suggesties/tips voor de dienst?"])

for nr in range(len(suggestions)):
  suggestions[nr] = alpha(filter_stopwords(suggestions[nr], stopwords)).lower()

for nr in range(len(all_sentences)):
  all_sentences[nr] = alpha(filter_stopwords(all_sentences[nr], stopwords)).lower()

for nr in range(len(test_set)):
  test_set['sentence'][nr] = alpha(filter_stopwords(test_set['sentence'][nr], stopwords)).lower()

"""**Multilingual model 1**"""

from sentence_transformers import SentenceTransformer, util
import torch

embedder = SentenceTransformer('xlm-r-bert-base-nli-stsb-mean-tokens')

#Make example sentence embeddings
multilingual_embeddings = embedder.encode(all_sentences, convert_to_numpy=True)
multilingual_embeddings.shape

#Make test set embeddings
input_sentences = list(test_set['sentence'])
input_embedding = embedder.encode(input_sentences, convert_to_numpy=True)
input_embeddings = np.reshape(input_embedding, (len(test_set['sentence']),768))
input_embeddings.shape

similarities = cosine_similarity(input_embeddings, multilingual_embeddings)

#Print neighbors and output
i = 0
for similarity in similarities[0:3]:
  neighbors = np.argsort(np.reshape(similarity, 64))[-3:][::-1]
  print("Input sentence: ", input_sentences[i], "\n")
  print("Most similar sentences:")

  #get categories
  categories = []
  k = 1
  for j in neighbors:
    if category[j] == 'geen' or category[j] == 'bugs' or category[j] == 'gesprek' or category[j] == 'advies' or category[j] == 'reactietijd':
      print(k, ".", "Similarity: ", round(similarities[i][j], 3), "\tCategory: ", category[j], "  \t\tSentence: ", all_sentences[j])
    else:
      print(k, ".", "Similarity: ", round(similarities[i][j], 3), "\tCategory: ", category[j], "\tSentence: ", all_sentences[j])
    categories.append(category[j])
    k += 1

  #calculate output
  output = count_categories(categories)
  print("\ny_pred = ", output)
  y = test_set['final_v2'][i]
  print("y_true = ", y, "\n\n")

  i += 1

#Get predictions
predictions = []
for similarity in similarities:
  neighbors = np.argsort(np.reshape(similarity, 64))[-3:][::-1]
  categories = []
  for j in neighbors:
    categories.append(category[j])
  x = count_categories(categories)
  predictions.append(x)

#Get evaluation measures
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

y_true = test_set['final_v2']
y_pred = predictions
labels = labels

cm = confusion_matrix(y_true, y_pred)#, labels = labels)
report = classification_report(y_true, y_pred)
print(report)

set(test_set['final_v2'])

#Plot confusion matrix
import seaborn as sns
import matplotlib.pyplot as plt     
from matplotlib.pyplot import figure
ax= plt.subplot()
sns.heatmap(cm, annot=True, ax = ax); #annot=True to annotate cells
sns.set(rc={'figure.figsize':(6,6)})

#Labels, title and ticks
ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels');
ax.set_title('Confusion Matrix'); 
ax.xaxis.set_ticklabels(labels, rotation = 89); ax.yaxis.set_ticklabels(labels, rotation = 0);

"""**Multilingual visualization 1**"""

#Dimensionality reduction
#source https://towardsdatascience.com/elmo-contextual-language-embedding-335de2268604
pca = PCA(n_components=50)
y = pca.fit_transform(multilingual_embeddings)
y = TSNE(n_components=2).fit_transform(y)

plot_data = pd.DataFrame({'x':y[:,0], 'y':y[:,1], 'Category':category})
ggplot(plot_data, aes(x='x', y='y', color='Category')) + geom_point(size = 3)

"""# **All sentences**

"""

len(suggestions) #there are 7832 sentences to classify

#Make example sentence embeddings
multilingual_embeddings = embedder.encode(all_sentences, convert_to_numpy=True)
multilingual_embeddings.shape

#Make all sentence embeddings
input_sentences = suggestions
input_embedding = embedder.encode(input_sentences, convert_to_numpy=True)
input_embeddings = np.reshape(input_embedding, (len(suggestions),768))
input_embeddings.shape

#Calculate similarities
similarities = cosine_similarity(input_embeddings, multilingual_embeddings)

#Print neighbors and output
i = 0
outputs = []
for similarity in similarities:
  neighbors = np.argsort(np.reshape(similarity, 64))[-3:][::-1]
  #print("Input sentence: ", input_sentences[i], "\n")
  #print("Most similar sentences:")


  #get categories
  categories = []
  k = 1
  for j in neighbors:
    #if category[j] == 'geen goed concept' or category[j] == 'goed gesprek/advies' or category[j] == 'goed concept':
      #print(k, ".", "Similarity: ", round(similarities[i][j], 3), "\tCategory: ", category[j], "\t\tSentence: ", all_sentences[j])
    #else:
      #print(k, ".", "Similarity: ", round(similarities[i][j], 3), "\tCategory: ", category[j], "\tSentence: ", all_sentences[j])
    categories.append(category[j])
    k += 1

  #calculate output
  output = count_categories(categories)
  outputs.append(output)
  #print("\ny_pred = ", output)
  #y = test_set['final_v2'][i]
  #print("y_true = ", y, "\n\n")
  i += 1

set(outputs)

value_counts

#get value counts
plot_data = pd.DataFrame({'x':outputs})
value_counts = plot_data['x'].value_counts()
df_value_counts = pd.DataFrame({'Category':['geen', 'tips', 'reactietijd', 'anders', 'bugs'], 'Count':[value for value in value_counts], 'Percentage':[round(value/len(outputs),2) for value in value_counts]})

df_value_counts

import seaborn as sns
import matplotlib.pyplot as plt     
from matplotlib.pyplot import figure
#colors = ["#FF0B04", "#4374B3"]
#customPalette = sns.set_palette(sns.color_palette(colors))
#sns.set_palette(customPalette)
sns.set_style("ticks")
sns.set_palette("Spectral")
sns.barplot(x='Category', y = 'Count', data = df_value_counts)
sns.set(rc={'figure.figsize':(10,10)})
sns.despine()
plt.xticks(rotation=90)

df_categorized = pd.DataFrame({'Sentence':suggestions, 'Category':outputs})

df_categorized

print(df_categorized.loc[df_categorized['Category'] == 'geen'][0:50])